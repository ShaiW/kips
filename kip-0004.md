```
  KIP: 4
  Layer: Consensus (hard fork), DAA
  Title: Sparse Difficulty Windows
  Author: Shai Wyborski <shai.wyborski@mail.huji.ac.il>
          Michael Sutton <msutton@cs.huji.ac.il>
          Georges Künzli <georges.kuenzli@dplanet.ch>
  Status: implemented in testnet ([PR](https://github.com/kaspanet/rusty-kaspa/pull/186))
```

# Motivation
The difficulty adjustment algorithm requires maintaining, for each block, a list of the N blocks in its past with the highest accumulated weight. When validating a block this list needs to be traversed to compute the various quantities required for computing the difficulty target. Since the length of the difficulty window corresponds to real-time considerations, the size of desired difficulty window is measured in seconds, not blocks. This means that if we increase the block rate by a factor of R, we increase the complexity of the difficulty adjustment by R^2 (the first factor is due to the fact that validating each block requires R times more steps, and the second factor is since blocks emerge R times faster).

This is partially mitigated by the incremental nature of the difficulty window. Like the blue set, each block inherits its selected parent's difficulty window and then fixes it according to the blocks in its anticone. This converts some of the CPU overhead to RAM overhead, which would also increase by a factor of R.

To reduce the overhead, the current Golang implementation uses window sizes that are not optimal (263 blocks for timestamp flexibility, 2641 blocks for DAA window). As a result, miners whose clock drifts into the future more than two minutes than the majority of the network have their block delayed decreasing the probability they will be blue. A secondary motivation for this proposal is to increase the timestamp flexibility (which requires increasing the DAA window accordingly) efficiently.

# Difficulty Windows

For the sake of discussion, we will call a window long/short to indicate the length of time it represents, and large/small to indicate the number of block it contains. In the current state, the size of a window is its length times the BPS. Thus, inceasing the BPS by a factor of R while retaining window *lengths* implieas increasing the *sizes* of all windows by a factor of R.

Currently, a window of a block B of length L is the set of L\*BPS blocks in the past of B with the highest blue score.

Difficulty windows are windows used for determining how to readjust the difficulty target. The difficulty adjustment algorithm actually uses two windows:
 * the *timestamp flexibility* window, used to bound the allowed deviation from the expected timestamp.
 * the *difficulty* window, used to approximate the deviation of block creation time from the expected time.

Timestamp flexibility length corresponds to natural deviations and drifts across different clocks of the network. Short timestamp flexibility window means the netowrk is less tolerable towards clock drifts and more punishing towards poorly connected miners. However, long timestamp flexibility allows an adversarial miner more leeway for timestamp manipulations (e.g. for the purpose of difficulty attack). This is countered by making the difficulty window much longer.

In the current implementations, efficiency considerations already force us to choose a shorter-than-desired timestamp flexibility length, namely 263 seconds. The corresponding difficulty window length is 2641 seconds. These lengths are already not optimal in two ways: the tolerated drift is too short, and the maximum possible difficulty attack via timestamp manipulation is higher than desired. However, efficiency concerns prohibit meaningfully extending the windows. (It should be noted that an optimal timestamp manipulation attack would require a lot of resources and luck to apply consistently, while only affecting the network mildly.)

Increasing the block rates while retaining window lengths will force us to increase window sizes accordingly, which is prohibitive in terms of complexity. Making the windows shorter is prohibitive in terms of security. Hence, finding a better way to represent a window is required.

# Sparse Windows

The idea of sparse window is simple: instead of using all blocks in the past, choose an appropriate subset of the past. If the subset is small enough, yet well distributed accross the non-sparse window, it could be used as a replacement. The subset should be:

 * Deterministic: it could be calculated from the DAG and would turn out the same for all nodes
 * Uniformly distributed: it should be well spread across the non-sparse window
 * Incremental: it should not depend on the point of view of a particular block, so it could be inherited by future blocks
 * Secure: a miner should not be able to cheaply affect the probability of their block to be sampled, the cost should be comparable to discarding a valid block

In the [previous proposal](kip-0003.md), we used the hash of the window block as a method to sample blocks. However, this approach turned out to be gameable. In the current proposal we consider a much simpler approach, defined by the following parameters:
* ``length``: the length of the window in seconds
* ``size``: the size of the window in blocks

From these we can define a new quantity ``sparsity = length*BPS/size``. Intuitively, we use "one out of every ``sparsity`` blocks". Note that ``sparsity`` is the only quantity affected by the BPS, and in particular setting ``sparsity=1`` recovers the current, non-sparse windows.

Also note that ``length`` and ``sparsity`` (and, implicity BPS) determine ``size``, so it is superfluous to specify ``size``. We thus refrain from using ``size`` again to avoid confusion between the sparse and non-sparse sizes.

Given the desired ``sparsity`` and ``length``, we define the sparse-window of a block B to be all blocks C in the past of B such that ``blue_score(C) > blue_score(B) - length*BPS`` and ``blue_score(C)

# Regulating Emissions

We currently regulate emissions by not rewarding blocks outside the difficulty window. This approach is unsuitable when the difficulty window is sampled. So instead, we only reward blocks whose accumulated blue score is at least as much as the lowest accumulated blue score witnessed in the difficulty window.

# Proposed Constants

We propose the following:

 * Increase the timestamp flexibility from two minutes to 10 minutes. This requires a window time of 20 minutes. I propose a sample rate of a block per 10 seconds. The overall size of the timestamp flexibility window would then be 121 blocks.
 * Increase the length of the difficulty window to 500 minutes, sampling a block per 30 seconds. The overall size of the difficulty window would be 1000 blocks.

In practice, the length of the windows and the sample rate would probably be slightly adjusted for simpler implementation. I am not explicating the adjustments as they depend on the block rate.

# Loose Ends

 * On a glance, it might seem worrisome that the bound on block rewards is probabilistic and has variance, since it might destabilize the emission schedule. However, since this only holds for red blocks that were not previously merged, the effect is marginal. Furthermore, the pruning protocol strongly limits the ability to merge old blocks, and the bound thereof will become more steep as we increase the length (in real world time) of the difficulty window. While I am positive this effect is negligible, we should measure it on the testnet before deploying to main. 
 * The DAA is currently retargeted based on the average difficulty across the difficulty window. This causes the difficulty adjustment to lag during times of greatly changing global hashrate. It might be better to use a different averaging, e.g. giving more weights to newer blocks (or just taking the average of a small subwindow). 
 * The median timestamp selection is much more sensitive to variance than the actual difficulty retargeting, we should make sure that the chosen constants do not incur problematic variance.

# Backwards compatibility
Breaks consensus rules, requires hardfork
